{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, unicode_literals\n",
    "\n",
    "import sys, os\n",
    "import io\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "import code\n",
    "import argparse\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import discoeval \n",
    "from examples.bert import batcher, prepare\n",
    "from discoeval.so import SPEval, BSOEval, DCEval, SSPEval\n",
    "\n",
    "from transformers import BertConfig, BertTokenizer, BertModel, BertForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-23 18:28:31,602 : Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2020-04-23 18:28:32,156 : https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/bert-base-uncased-config.json HTTP/1.1\" 200 0\n",
      "2020-04-23 18:28:32,165 : loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/daniter/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "2020-04-23 18:28:32,181 : Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2020-04-23 18:28:32,188 : Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2020-04-23 18:28:32,590 : https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/bert-base-uncased-vocab.txt HTTP/1.1\" 200 0\n",
      "2020-04-23 18:28:32,598 : loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/daniter/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2020-04-23 18:28:32,785 : ***** Transfer task : Binary Sentece Ordering. Task name: BSO wiki *****\n",
      "\n",
      "\n",
      "2020-04-23 18:28:32,989 : Loaded 10000 instances\n",
      "\n",
      "2020-04-23 18:28:33,037 : Loaded 4000 instances\n",
      "\n",
      "2020-04-23 18:28:33,076 : Loaded 4000 instances\n",
      "\n",
      "2020-04-23 18:28:33,727 : Computed train embeddings, shape: (20000, 768)\n",
      "2020-04-23 18:28:33,818 : Computed valid embeddings, shape: (8000, 768)\n",
      "2020-04-23 18:28:33,884 : Computed test embeddings, shape: (8000, 768)\n",
      "2020-04-23 18:28:33,885 : Training pytorch-MLP-nhid0-adam-bs64 with standard validation..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANITER False\n",
      "DANITER cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-23 18:28:41,313 : [('reg:1e-09', 70.53)]\n",
      "2020-04-23 18:28:41,315 : Validation : best param found is reg = 1e-09 with score             70.53\n",
      "2020-04-23 18:28:41,319 : Evaluating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANITER False\n",
      "DANITER cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-23 18:28:48,342 : \n",
      "Task: BSO wiki, Dev acc : 70.53 Test acc : 72.7 for             Binary Sentence Ordering prediction\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2615 1385]\n",
      " [ 799 3201]]\n",
      "{'BSOwiki': {'devacc': 70.53, 'acc': 72.7, 'ndev': 8000, 'ntest': 8000}}\n"
     ]
    }
   ],
   "source": [
    "# Set up logger\n",
    "PATH_TO_DATA = \"../data/\"\n",
    "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.DEBUG)\n",
    "config = BertConfig.from_pretrained('bert-{}-uncased'.format(\"base\"))\n",
    "config.output_hidden_states = True\n",
    "config.output_attentions = True\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-{}-uncased'.format(\"base\"))\n",
    "\n",
    "# model = BertModel.from_pretrained('../models/conpono/bert/', config=config).cuda()\n",
    "# model.eval()\n",
    "\n",
    "# Set params for SentEval\n",
    "params_discoeval = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 10, 'batch_size': 8,\n",
    "                    'tokenizer': tokenizer, \"layer\": \"pooler\", \"model\": \"base\", \"seed\": 111}\n",
    "params_discoeval['classifier'] = {'nhid': 0, 'optim': 'adam', 'batch_size': 64,\n",
    "                                 'tenacity': 5, 'epoch_size': 4}\n",
    "se = discoeval.engine.SE(params_discoeval, batcher, prepare)\n",
    "\n",
    "\n",
    "transfer_tasks = [\n",
    "    ['SParxiv'],\n",
    "    ['BSOwiki'],\n",
    "    ['DCwiki']]\n",
    "    #['SParxiv', 'SProc', 'SPwiki'], \n",
    "    #['DCchat', 'DCwiki'],\n",
    "    #['BSOarxiv', 'BSOroc', 'BSOwiki'], \n",
    "    #['SSPabs', 'RST'],\n",
    "    #[ 'PDTB-E', 'PDTB-I']]\n",
    "    \n",
    "#se.evaluation = SPEval(os.path.join(PATH_TO_DATA, 'SP/arxiv'), \"Sentence Position arXiv\", nclasses=5, seed=111)\n",
    "#se.params.current_task = transfer_tasks[0][0]\n",
    "#se.evaluation.do_prepare(se.params, se.prepare)\n",
    "results = se.eval(transfer_tasks[1])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pred = np.load(\"bert_predictions.npy\").ravel()\n",
    "conpono_pred = np.load(\"conpono_predictions.npy\").ravel()\n",
    "labels = np.load(\"labels.npy\").xravel()\n",
    "\n",
    "conpono_true = np.argwhere(conpono_pred == labels) \n",
    "bert_false = np.argwhere(bert_pred != labels) \n",
    "improvement = np.intersect1d(conpono_true, bert_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = set([\"but\", \"and\", \"as\", \"when\", \"if\", \"before\", \"because\",\n",
    "          \"while\", \"though\", \"after\", \"so\", \"although\", \"then\",\n",
    "          \"also\", \"still\"])\n",
    "\n",
    "#markers = set([\"but\", \"when\", \"if\", \"before\", \"because\",\n",
    "     #     \"while\", \"though\", \"after\", \"so\", \"although\", \"then\",\n",
    "     #     \"also\", \"still\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The district lies east of the Melbourne and nestled between the Strzelecki Ranges to the south and the Baw Baw Ranges , part of the Great Dividing Range , to the north .\n",
      "-\n",
      "Mount Baw Baw ( ) is the highest peak to the north of the Latrobe Valley , due north of Moe .\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "He was briefly imprisoned by Henry VII , but later restored to his position .\n",
      "-\n",
      "A few years later he was murdered by citizens of York during a revolt against Henry VII 's taxation .\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Oncken baptized these believers and established a church in Copenhagen in that year .\n",
      "-\n",
      "Until 1849 , when religious liberty was granted through the Constitution of 1849 , Baptists were fined , imprisoned , and their infants baptized by compulsion .\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "all_vecs_p = np.load(\"../tmp/BSO-BSO wiki-conpono-test-x.npy\")\n",
    "all_vecs_n = np.load(\"../tmp/BSO-BSO wiki-conpono-test-y.npy\")\n",
    "all_vecs = np.concatenate((all_vecs_p, all_vecs_n) )\n",
    "\n",
    "for i in random.sample(list(improvement), 4):\n",
    "    # print(i)\n",
    "    bert_vec = se.evaluation.embed['test']['X'][i]\n",
    "    for j in range(all_vecs.shape[0]):\n",
    "        if (all_vecs[j] == bert_vec).all():\n",
    "            sents = se.evaluation.data['test']['X'][j % 4000]\n",
    "            words = set(sents[0] + sents[1])\n",
    "            if words.intersection(markers):\n",
    "                sents = [\" \".join(s) for s in sents]\n",
    "                print(sents[0])\n",
    "                print(\"-\")\n",
    "                print(sents[1])\n",
    "            \n",
    "    print(\"~\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1241\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "all_vecs_p = np.load(\"../tmp/BSO-BSO wiki-conpono-test-x.npy\")\n",
    "all_vecs_n = np.load(\"../tmp/BSO-BSO wiki-conpono-test-y.npy\")\n",
    "all_vecs = np.concatenate((all_vecs_p, all_vecs_n) )\n",
    "\n",
    "word_counter = Counter()\n",
    "print(len(improvement))\n",
    "cc = 0\n",
    "indexes_to_skip = []\n",
    "num_words = 0\n",
    "for i in list(improvement):# random.sample(list(improvement), 4):\n",
    "    # print(i)\n",
    "    cc += 1\n",
    "    if cc % 100 == 0:\n",
    "        print(cc)\n",
    "    bert_vec = se.evaluation.embed['test']['X'][i]\n",
    "    for j in range(all_vecs.shape[0]):\n",
    "        if (all_vecs[j] == bert_vec).all():\n",
    "            sents = se.evaluation.data['test']['X'][j % 4000]\n",
    "            words = set([w.lower() for w in sents[0] + sents[1]])\n",
    "            num_words += len(words)\n",
    "            indexes_to_skip.append(j)\n",
    "            if words.intersection(markers):\n",
    "                for w in words.intersection(markers):\n",
    "                    word_counter[w] += 1\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and \t\t 0.01954220376931939\n",
      "as \t\t 0.009173314783818446\n",
      "also \t\t 0.0030432797861009065\n",
      "but \t\t 0.0018911810099341347\n",
      "after \t\t 0.0017172793078712258\n",
      "when \t\t 0.001652066169597635\n",
      "before \t\t 0.0011520987761667718\n",
      "then \t\t 0.000847770797556681\n",
      "while \t\t 0.0008042953720409538\n",
      "although \t\t 0.000739082233767363\n",
      "so \t\t 0.00047822968067299956\n",
      "because \t\t 0.00045649196791513596\n",
      "still \t\t 0.0002608525530943634\n",
      "if \t\t 0.00023911484033649978\n",
      "though \t\t 0.00023911484033649978\n"
     ]
    }
   ],
   "source": [
    "for w, c in word_counter.most_common():\n",
    "    print(w, \"\\t\\t\",c/ num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_word_counter = Counter()\n",
    "general_num_words = 0\n",
    "for j in range(len(se.evaluation.data['test']['X'])):\n",
    "    if j in indexes_to_skip:\n",
    "        continue\n",
    "    sents = se.evaluation.data['test']['X'][j]\n",
    "    words = set([w.lower() for w in sents[0] + sents[1]])\n",
    "    general_num_words += len(words)\n",
    "    if words.intersection(markers):\n",
    "        for w in words.intersection(markers):\n",
    "            general_word_counter[w] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and \t\t 0.019658636272054802\n",
      "as \t\t 0.008958639150332442\n",
      "also \t\t 0.0027487551449212793\n",
      "after \t\t 0.0017485536654865729\n",
      "but \t\t 0.0014103560429439023\n",
      "when \t\t 0.0013383991019773767\n",
      "before \t\t 0.0008778746797916127\n",
      "then \t\t 0.0007339607978585614\n",
      "while \t\t 0.0006404167746020781\n",
      "although \t\t 0.000575655527732205\n",
      "because \t\t 0.00038856748121923843\n",
      "so \t\t 0.00029502345796275506\n",
      "if \t\t 0.00028063206976944993\n",
      "still \t\t 0.0002734363756727974\n",
      "though \t\t 0.00025184929338283973\n"
     ]
    }
   ],
   "source": [
    "for w, c in general_word_counter.most_common():\n",
    "    print(w, \"\\t\\t\",c/ general_num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and \t\t -0.005922715142805883\n",
      "as \t\t 0.023962973603869057\n",
      "also \t\t 0.107148372863914\n",
      "but \t\t 0.34092452710493154\n",
      "after \t\t -0.01788584373053501\n",
      "when \t\t 0.2343598909748523\n",
      "before \t\t 0.31237271411023443\n",
      "then \t\t 0.1550627772357557\n",
      "while \t\t 0.2558936679019712\n",
      "although \t\t 0.2838967023889745\n",
      "so \t\t 0.6209886629875147\n",
      "because \t\t 0.17480744009448648\n",
      "still \t\t -0.04602102608868766\n",
      "if \t\t -0.14794185663476792\n",
      "though \t\t -0.05056378310731301\n"
     ]
    }
   ],
   "source": [
    "counter = Counter()\n",
    "for w,c in word_counter.most_common():\n",
    "    gc = general_word_counter[w]\n",
    "    tmp_c = c / num_words\n",
    "    tmp_gc = gc / general_num_words\n",
    "    print(w, \"\\t\\t\", (tmp_c - tmp_gc) / tmp_gc)\n",
    "    counter[w] = (tmp_c - tmp_gc) / tmp_gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1494055003041596"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(list(counter.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1241\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "also \t\t 0.015531681979754594\n",
      "when \t\t 0.3851946571513812\n",
      "but \t\t 0.26822266388082017\n",
      "after \t\t 0.009700351628191396\n",
      "before \t\t 0.01910749776137334\n",
      "although \t\t 0.37755220387054583\n",
      "then \t\t 0.056852219900683454\n",
      "while \t\t -0.0298096621311726\n",
      "so \t\t 0.48878312716444106\n",
      "because \t\t 0.02913150593607541\n",
      "though \t\t 0.20147410262393492\n",
      "still \t\t -0.0012746521938540017\n",
      "if \t\t -0.18471400179090133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12055290164812194"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_pred = np.load(\"bert_predictions.npy\").ravel()\n",
    "conpono_pred = np.load(\"conpono_predictions.npy\").ravel()\n",
    "labels = np.load(\"labels.npy\").ravel()\n",
    "\n",
    "conpono_true = np.argwhere(conpono_pred == labels) \n",
    "bert_false = np.argwhere(bert_pred != labels) \n",
    "improvement = np.intersect1d(conpono_true, bert_false)\n",
    "\n",
    "markers = set([\"but\", \"and\", \"as\", \"when\", \"if\", \"before\", \"because\",\n",
    "          \"while\", \"though\", \"after\", \"so\", \"although\", \"then\",\n",
    "          \"also\", \"still\"])\n",
    "\n",
    "markers = set([\"but\", \"when\", \"if\", \"before\", \"because\",\n",
    "          \"while\", \"though\", \"after\", \"so\", \"although\", \"then\",\n",
    "          \"also\", \"still\"])\n",
    "\n",
    "from collections import Counter\n",
    "all_vecs_p = np.load(\"../tmp/BSO-BSO wiki-conpono-test-x.npy\")\n",
    "all_vecs_n = np.load(\"../tmp/BSO-BSO wiki-conpono-test-y.npy\")\n",
    "all_vecs = np.concatenate((all_vecs_p, all_vecs_n) )\n",
    "\n",
    "word_counter = Counter()\n",
    "word2example = defaultdict(list)\n",
    "print(len(improvement))\n",
    "cc = 0\n",
    "indexes_to_skip = []\n",
    "num_words = 0\n",
    "for i in list(improvement):\n",
    "    # print(i)\n",
    "    cc += 1\n",
    "    if cc % 100 == 0:\n",
    "        print(cc)\n",
    "    bert_vec = se.evaluation.embed['test']['X'][i]\n",
    "    for j in range(all_vecs.shape[0]):\n",
    "        if (all_vecs[j] == bert_vec).all():\n",
    "            sents = se.evaluation.data['test']['X'][j % 4000]\n",
    "            words = set([w.lower() for w in sents[0] ])\n",
    "            num_words += len(words)\n",
    "            indexes_to_skip.append(j)\n",
    "            if words.intersection(markers):\n",
    "                for w in words.intersection(markers):\n",
    "                    word_counter[w] += 1\n",
    "                    word2example[w].append([\" \".join(sents[0]), \" \".join(sents[1])])\n",
    "            break\n",
    "            \n",
    "general_word_counter = Counter()\n",
    "general_num_words = 0\n",
    "for j in range(len(se.evaluation.data['test']['X'])):\n",
    "#     if j in indexes_to_skip:\n",
    "#         continue\n",
    "    sents = se.evaluation.data['test']['X'][j]\n",
    "    words = set([w.lower() for w in sents[0] + sents[1]])\n",
    "    general_num_words += len(words)\n",
    "    if words.intersection(markers):\n",
    "        for w in words.intersection(markers):\n",
    "            general_word_counter[w] += 1\n",
    "\n",
    "counter = Counter()\n",
    "for w,c in word_counter.most_common():\n",
    "    gc = general_word_counter[w]\n",
    "    tmp_c = c / num_words\n",
    "    tmp_gc = gc / general_num_words\n",
    "    print(w, \"\\t\\t\", (tmp_c - tmp_gc) / tmp_gc)\n",
    "    counter[w] = (tmp_c - tmp_gc) / tmp_gc\n",
    "np.average([c for w,c in counter.most_common()], weights=[general_word_counter[w] for w,c in counter.most_common()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "also \t\t 0.24299598343426776\n",
      "after \t\t -0.012176749703440137\n",
      "but \t\t 0.02340002636088041\n",
      "when \t\t -0.2691364636239045\n",
      "while \t\t 0.3284519572953735\n",
      "then \t\t -0.015961513114537994\n",
      "because \t\t 0.5970460688796844\n",
      "although \t\t 0.01797084850220205\n",
      "before \t\t -0.43066344687341124\n",
      "still \t\t 0.7712692763938316\n",
      "so \t\t 0.5402341533859405\n",
      "if \t\t 0.6869231203750777\n",
      "though \t\t 0.16530873446962616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08176833661038796"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_pred = np.load(\"bert_predictions.npy\").ravel()\n",
    "conpono_pred = np.load(\"conpono_predictions.npy\").ravel()\n",
    "labels = np.load(\"labels.npy\").ravel()\n",
    "\n",
    "conpono_true = np.argwhere(conpono_pred != labels) \n",
    "bert_false = np.argwhere(bert_pred == labels) \n",
    "regression = np.intersect1d(conpono_true, bert_false)\n",
    "\n",
    "markers = set([\"but\", \"and\", \"as\", \"when\", \"if\", \"before\", \"because\",\n",
    "          \"while\", \"though\", \"after\", \"so\", \"although\", \"then\",\n",
    "          \"also\", \"still\"])\n",
    "\n",
    "markers = set([\"but\", \"when\", \"if\", \"before\", \"because\",\n",
    "          \"while\", \"though\", \"after\", \"so\", \"although\", \"then\",\n",
    "          \"also\", \"still\"])\n",
    "\n",
    "from collections import Counter\n",
    "all_vecs_p = np.load(\"../tmp/BSO-BSO wiki-conpono-test-x.npy\")\n",
    "all_vecs_n = np.load(\"../tmp/BSO-BSO wiki-conpono-test-y.npy\")\n",
    "all_vecs = np.concatenate((all_vecs_p, all_vecs_n) )\n",
    "\n",
    "word_counter = Counter()\n",
    "print(len(regression))\n",
    "cc = 0\n",
    "indexes_to_skip = []\n",
    "num_words = 0\n",
    "for i in list(regression):\n",
    "    # print(i)\n",
    "    cc += 1\n",
    "    if cc % 100 == 0:\n",
    "        print(cc)\n",
    "    bert_vec = se.evaluation.embed['test']['X'][i]\n",
    "    for j in range(all_vecs.shape[0]):\n",
    "        if (all_vecs[j] == bert_vec).all():\n",
    "            sents = se.evaluation.data['test']['X'][j % 4000]\n",
    "            words = set([w.lower() for w in sents[1]])\n",
    "            num_words += len(words)\n",
    "            indexes_to_skip.append(j)\n",
    "            if words.intersection(markers):\n",
    "                for w in words.intersection(markers):\n",
    "                    word_counter[w] += 1\n",
    "            break\n",
    "            \n",
    "general_word_counter = Counter()\n",
    "general_num_words = 0\n",
    "for j in range(len(se.evaluation.data['test']['X'])):\n",
    "#     if j in indexes_to_skip:\n",
    "#         continue\n",
    "    sents = se.evaluation.data['test']['X'][j]\n",
    "    words = set([w.lower() for w in sents[0] + sents[1]])\n",
    "    general_num_words += len(words)\n",
    "    if words.intersection(markers):\n",
    "        for w in words.intersection(markers):\n",
    "            general_word_counter[w] += 1\n",
    "\n",
    "counter = Counter()\n",
    "for w,c in word_counter.most_common():\n",
    "    gc = general_word_counter[w]\n",
    "    tmp_c = c / num_words\n",
    "    tmp_gc = gc / general_num_words\n",
    "    print(w, \"\\t\\t\", (tmp_c - tmp_gc) / tmp_gc)\n",
    "    counter[w] = (tmp_c - tmp_gc) / tmp_gc\n",
    "np.average([c for w,c in counter.most_common()], weights=[general_word_counter[w] for w,c in counter.most_common()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when\n",
      "Relevance is the concept of one topic being connected to another topic in a way that makes it useful to consider the second topic when considering the first .\n",
      "-\n",
      "The concept of relevance is studied in many different fields , including cognitive sciences , logic , and library and information science .\n",
      "\n",
      "The skull remained unstudied until 1956 when Francis Fraser examined it and concluded that it was similar to species in both the genera `` Lagenorhynchus '' and `` Delphinus '' but not the same as either .\n",
      "-\n",
      "A new genus was created by simply merging these two names together .\n",
      "\n",
      "Among the earliest references to music from Catalonia date to the Middle Ages , when Barcelona and the surrounding area were relatively prosperous , allowing both music and arts to be cultivated actively .\n",
      "-\n",
      "Catalonia and adjacent areas were the home for several troubadours , the itinerant composer-musicians whose influence and aesthetics was decisive on the formation of late medieval secular music , and who traveled into Italy and Northern France after the destruction of Occitann culture by the Albigensian Crusade in the early 13th century .\n",
      "\n",
      "It is invoked when debating the use of an intervention that carries an obvious risk of harm but a less certain chance of benefit .\n",
      "-\n",
      "Non-maleficence is often contrasted with its corollary , beneficence .\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "but\n",
      "Richard himself when young was suspected of plotting rebellion and imprisoned , but in later life was a staunch supporter of the Crown , which rewarded him richly for his loyalty .\n",
      "-\n",
      "The fifth Earl was a Major-General in the British Army .\n",
      "\n",
      "As a result , the `` Mars Surveyor 2001 Lander '' was canceled in May 2000 , but the decision was made to go ahead with its orbiter counterpart .\n",
      "-\n",
      "The `` Mars Surveyor 2001 Orbiter '' , renamed `` 2001 Mars Odyssey '' , was launched April 7 , 2001 reached Mars on October 24 , 2001 .\n",
      "\n",
      "Ludvigsen lost her father at the age of fourteen , but her mother Karen managed the very popular radio lessons until 1953 and provided for her family by numerous translations of classic and modern French literature .\n",
      "-\n",
      "Ludvigsen married in 1946 to Holger Ludvigsen ( 1925–2008 ) and never finished her university studies in literature , but became a mother of five children .\n",
      "\n",
      "AfterStep originally was a variant of FVWM modified to resemble NeXTSTEP , but as the development cycle progressed , it diverged from its FVWM roots .\n",
      "-\n",
      "In 2000 , Linux website TuxRadar selected AfterStep as one of the year 's best window managers , praising it as `` fast and reliable , with a huge range of configuration options and the ability to create some spectacular themes '' .\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "although\n",
      "Although critique is commonly understood as fault finding and negative judgment , it can also involve merit recognition , and in the philosophical tradition it also means a methodical practice of doubt .\n",
      "-\n",
      "The contemporary sense of critique has been largely influenced by the Enlightenment critique of prejudice and authority , which championed the emancipation and autonomy from religious and political authorities .\n",
      "\n",
      "Although both titles Marth appears in prior to `` '' were released exclusively in Japan , he acquired more widespread international attention through his recurring appearances in Nintendo 's `` Super Smash Bros. '' series of fighting games .\n",
      "-\n",
      "Marth and 's appearances increased western interest in the `` Fire Emblem '' series , and in part it led Nintendo to start releasing the games internationally , beginning with Fire Emblem , the seventh installment in the series , released to the west under the title `` Fire Emblem '' .\n",
      "\n",
      "Although different schools vary in the extent to which they emphasize each , both DSW and PhD candidates in the field of social work gain experience in education , advanced practice , teaching , supervision , research , policy analysis , administration and/or program development .\n",
      "-\n",
      "As with other doctorates , a holder of a DSW is referred to using the formal title of 'doctor ' .\n",
      "\n",
      "Although it lasted only twenty-three years ( 1933–1956 ) and enrolled fewer than 1,200 students , Black Mountain College was one of the most fabled experimental institutions in art education and practice .\n",
      "-\n",
      "It launched a remarkable number of the artists who spearheaded the avant-garde in the America of the 1960s .\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "for w in [\"when\", \"but\", \"although\"]:\n",
    "    print(w)\n",
    "    for e in random.sample(word2example[w], 4):\n",
    "        print(e[0])\n",
    "        print(\"-\")\n",
    "        print(e[1])\n",
    "        print(\"\")\n",
    "    print(\"~~\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples \n",
    "\n",
    "In 1941 Vaughn joined the United States National Guard for what had been planned as a one-year assignment , but when World War II broke out , he was sent abroad until the war ended in 1945 .\n",
    "\n",
    "He decided to make music a career when he was discharged from the army at the end of the war , and attended Western Kentucky State College , now known as Western Kentucky University , majoring in music composition ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1\n",
      "500\n",
      "1000\n",
      "2\n",
      "500\n",
      "1000\n",
      "3\n",
      "500\n",
      "1000\n",
      "4\n",
      "500\n",
      "1000\n",
      "5\n",
      "500\n",
      "1000\n",
      "6\n",
      "500\n",
      "1000\n",
      "7\n",
      "500\n",
      "1000\n",
      "8\n",
      "500\n",
      "1000\n",
      "9\n",
      "500\n",
      "1000\n",
      "0.00028860140393729505 0.009065909787388653\n"
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "for repetition in range(10):\n",
    "    print(repetition)\n",
    "    bert_pred = np.load(\"bert_predictions.npy\").ravel()\n",
    "    conpono_pred = np.load(\"conpono_predictions.npy\").ravel()\n",
    "    labels = np.load(\"labels.npy\").ravel()\n",
    "\n",
    "    random_samples = np.random.choice(len(labels), len(improvement), replace=False)\n",
    "\n",
    "    markers = set([\"but\", \"and\", \"as\", \"when\", \"if\", \"before\", \"because\",\n",
    "              \"while\", \"though\", \"after\", \"so\", \"although\", \"then\",\n",
    "              \"also\", \"still\"])\n",
    "\n",
    "    from collections import Counter\n",
    "    all_vecs_p = np.load(\"../tmp/BSO-BSO wiki-conpono-test-x.npy\")\n",
    "    all_vecs_n = np.load(\"../tmp/BSO-BSO wiki-conpono-test-y.npy\")\n",
    "    all_vecs = np.concatenate((all_vecs_p, all_vecs_n) )\n",
    "\n",
    "    word_counter = Counter()\n",
    "    #print(len(random_samples))\n",
    "    cc = 0\n",
    "    indexes_to_skip = []\n",
    "    num_words = 0\n",
    "    for i in list(random_samples):\n",
    "        # print(i)\n",
    "        cc += 1\n",
    "        if cc % 500 == 0:\n",
    "            print(cc)\n",
    "        bert_vec = se.evaluation.embed['test']['X'][i]\n",
    "        for j in range(all_vecs.shape[0]):\n",
    "            if (all_vecs[j] == bert_vec).all():\n",
    "                sents = se.evaluation.data['test']['X'][j % 4000]\n",
    "                words = set([w.lower() for w in sents[0] + sents[1]])\n",
    "                num_words += len(words)\n",
    "                indexes_to_skip.append(j)\n",
    "                if words.intersection(markers):\n",
    "                    for w in words.intersection(markers):\n",
    "                        word_counter[w] += 1\n",
    "                break\n",
    "\n",
    "    general_word_counter = Counter()\n",
    "    general_num_words = 0\n",
    "    for j in range(len(se.evaluation.data['test']['X'])):\n",
    "    #     if j in indexes_to_skip:\n",
    "    #         continue\n",
    "        sents = se.evaluation.data['test']['X'][j]\n",
    "        words = set([w.lower() for w in sents[0] + sents[1]])\n",
    "        general_num_words += len(words)\n",
    "        if words.intersection(markers):\n",
    "            for w in words.intersection(markers):\n",
    "                general_word_counter[w] += 1\n",
    "\n",
    "    counter = Counter()\n",
    "    for w,c in word_counter.most_common():\n",
    "        gc = general_word_counter[w]\n",
    "        tmp_c = c / num_words\n",
    "        tmp_gc = gc / general_num_words\n",
    "        #print(w, \"\\t\\t\", (tmp_c - tmp_gc) / tmp_gc)\n",
    "        counter[w] = (tmp_c - tmp_gc) / tmp_gc\n",
    "    stats.append(np.average([c for w,c in counter.most_common()], weights=[general_word_counter[w] for w,c in counter.most_common()]))\n",
    "print(np.mean(stats), np.std(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "and \t\t 0.15935676489515455\n",
      "as \t\t -0.02397053388410366\n",
      "also \t\t 0.015531681979754594\n",
      "when \t\t 0.3851946571513812\n",
      "but \t\t 0.26822266388082017\n",
      "after \t\t 0.009700351628191396\n",
      "before \t\t 0.01910749776137334\n",
      "although \t\t 0.37755220387054583\n",
      "then \t\t 0.056852219900683454\n",
      "while \t\t -0.0298096621311726\n",
      "so \t\t 0.48878312716444106\n",
      "because \t\t 0.02913150593607541\n",
      "though \t\t 0.20147410262393492\n",
      "still \t\t -0.0012746521938540017\n",
      "if \t\t -0.18471400179090133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11807586178615496"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_pred = np.load(\"bert_predictions.npy\").ravel()\n",
    "conpono_pred = np.load(\"conpono_predictions.npy\").ravel()\n",
    "labels = np.load(\"labels.npy\").ravel()\n",
    "\n",
    "conpono_true = np.argwhere(conpono_pred == labels) \n",
    "bert_false = np.argwhere(bert_pred != labels) \n",
    "improvement = np.intersect1d(conpono_true, bert_false)\n",
    "\n",
    "markers = set([\"but\", \"and\", \"as\", \"when\", \"if\", \"before\", \"because\",\n",
    "          \"while\", \"though\", \"after\", \"so\", \"although\", \"then\",\n",
    "          \"also\", \"still\"])\n",
    "from collections import Counter\n",
    "all_vecs_p = np.load(\"../tmp/BSO-BSO wiki-conpono-test-x.npy\")\n",
    "all_vecs_n = np.load(\"../tmp/BSO-BSO wiki-conpono-test-y.npy\")\n",
    "all_vecs = np.concatenate((all_vecs_p, all_vecs_n) )\n",
    "\n",
    "word_counter = Counter()\n",
    "cc = 0\n",
    "indexes_to_skip = []\n",
    "num_words = 0\n",
    "for i in list(improvement):\n",
    "    # print(i)\n",
    "    cc += 1\n",
    "    if cc % 100 == 0:\n",
    "        print(cc)\n",
    "    bert_vec = se.evaluation.embed['test']['X'][i]\n",
    "    for j in range(all_vecs.shape[0]):\n",
    "        if (all_vecs[j] == bert_vec).all():\n",
    "            sents = se.evaluation.data['test']['X'][j % 4000]\n",
    "            words = set([w.lower() for w in sents[0]])\n",
    "            num_words += len(words)\n",
    "            indexes_to_skip.append(j)\n",
    "            if words.intersection(markers):\n",
    "                for w in words.intersection(markers):\n",
    "                    word_counter[w] += 1\n",
    "            break\n",
    "            \n",
    "general_word_counter = Counter()\n",
    "general_num_words = 0\n",
    "for j in range(len(se.evaluation.data['test']['X'])):\n",
    "#     if j in indexes_to_skip:\n",
    "#         continue\n",
    "    sents = se.evaluation.data['test']['X'][j]\n",
    "    words = set([w.lower() for w in sents[0] + sents[1]])\n",
    "    general_num_words += len(words)\n",
    "    if words.intersection(markers):\n",
    "        for w in words.intersection(markers):\n",
    "            general_word_counter[w] += 1\n",
    "\n",
    "counter = Counter()\n",
    "for w,c in word_counter.most_common():\n",
    "    gc = general_word_counter[w]\n",
    "    tmp_c = c / num_words\n",
    "    tmp_gc = gc / general_num_words\n",
    "    print(w, \"\\t\\t\", (tmp_c - tmp_gc) / tmp_gc)\n",
    "    counter[w] = (tmp_c - tmp_gc) / tmp_gc\n",
    "np.mean(list(counter.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flipped 961\n",
      "Correct 280\n",
      "Counter({'only_flipped': 854, 'only_correct': 173, 'both': 107})\n"
     ]
    }
   ],
   "source": [
    "print(\"Flipped\", (np.array(indexes_to_skip) > 4000).sum())\n",
    "print(\"Correct\", (np.array(indexes_to_skip) < 4000).sum())\n",
    "\n",
    "flip_counter = Counter()\n",
    "for i in indexes_to_skip:\n",
    "    if i < 4000:\n",
    "        if (i + 4000) in indexes_to_skip:\n",
    "            flip_counter[\"both\"] += 1\n",
    "        else:\n",
    "            flip_counter[\"only_correct\"] += 1\n",
    "    else:\n",
    "        if (i % 4000) not in indexes_to_skip:\n",
    "            flip_counter[\"only_flipped\"] += 1\n",
    "print(flip_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "and \t\t 0.1933654153542571\n",
      "as \t\t -0.03433415103248922\n",
      "also \t\t 0.24299598343426776\n",
      "after \t\t -0.012176749703440137\n",
      "but \t\t 0.02340002636088041\n",
      "when \t\t -0.2691364636239045\n",
      "while \t\t 0.3284519572953735\n",
      "then \t\t -0.015961513114537994\n",
      "because \t\t 0.5970460688796844\n",
      "although \t\t 0.01797084850220205\n",
      "before \t\t -0.43066344687341124\n",
      "still \t\t 0.7712692763938316\n",
      "so \t\t 0.5402341533859405\n",
      "if \t\t 0.6869231203750777\n",
      "though \t\t 0.16530873446962616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.18697955067355723"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_pred = np.load(\"bert_predictions.npy\").ravel()\n",
    "conpono_pred = np.load(\"conpono_predictions.npy\").ravel()\n",
    "labels = np.load(\"labels.npy\").ravel()\n",
    "\n",
    "conpono_true = np.argwhere(conpono_pred != labels) \n",
    "bert_false = np.argwhere(bert_pred == labels) \n",
    "regression = np.intersect1d(conpono_true, bert_false)\n",
    "\n",
    "markers = set([\"but\", \"and\", \"as\", \"when\", \"if\", \"before\", \"because\",\n",
    "          \"while\", \"though\", \"after\", \"so\", \"although\", \"then\",\n",
    "          \"also\", \"still\"])\n",
    "from collections import Counter\n",
    "all_vecs_p = np.load(\"../tmp/BSO-BSO wiki-conpono-test-x.npy\")\n",
    "all_vecs_n = np.load(\"../tmp/BSO-BSO wiki-conpono-test-y.npy\")\n",
    "all_vecs = np.concatenate((all_vecs_p, all_vecs_n) )\n",
    "\n",
    "word_counter = Counter()\n",
    "cc = 0\n",
    "indexes_to_skip = []\n",
    "num_words = 0\n",
    "for i in list(regression):\n",
    "    # print(i)\n",
    "    cc += 1\n",
    "    if cc % 100 == 0:\n",
    "        print(cc)\n",
    "    bert_vec = se.evaluation.embed['test']['X'][i]\n",
    "    for j in range(all_vecs.shape[0]):\n",
    "        if (all_vecs[j] == bert_vec).all():\n",
    "            sents = se.evaluation.data['test']['X'][j % 4000]\n",
    "            words = set([w.lower() for w in sents[1]])\n",
    "            num_words += len(words)\n",
    "            indexes_to_skip.append(j)\n",
    "            if words.intersection(markers):\n",
    "                for w in words.intersection(markers):\n",
    "                    word_counter[w] += 1\n",
    "            break\n",
    "            \n",
    "general_word_counter = Counter()\n",
    "general_num_words = 0\n",
    "for j in range(len(se.evaluation.data['test']['X'])):\n",
    "#     if j in indexes_to_skip:\n",
    "#         continue\n",
    "    sents = se.evaluation.data['test']['X'][j]\n",
    "    words = set([w.lower() for w in sents[0] + sents[1]])\n",
    "    general_num_words += len(words)\n",
    "    if words.intersection(markers):\n",
    "        for w in words.intersection(markers):\n",
    "            general_word_counter[w] += 1\n",
    "\n",
    "counter = Counter()\n",
    "for w,c in word_counter.most_common():\n",
    "    gc = general_word_counter[w]\n",
    "    tmp_c = c / num_words\n",
    "    tmp_gc = gc / general_num_words\n",
    "    print(w, \"\\t\\t\", (tmp_c - tmp_gc) / tmp_gc)\n",
    "    counter[w] = (tmp_c - tmp_gc) / tmp_gc\n",
    "np.mean(list(counter.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flipped 167\n",
      "Correct 632\n",
      "Counter({'only_correct': 570, 'only_flipped': 105, 'both': 62})\n"
     ]
    }
   ],
   "source": [
    "print(\"Flipped\", (np.array(indexes_to_skip) > 4000).sum())\n",
    "print(\"Correct\", (np.array(indexes_to_skip) < 4000).sum())\n",
    "\n",
    "flip_counter = Counter()\n",
    "for i in indexes_to_skip:\n",
    "    if i < 4000:\n",
    "        if (i + 4000) in indexes_to_skip:\n",
    "            flip_counter[\"both\"] += 1\n",
    "        else:\n",
    "            flip_counter[\"only_correct\"] += 1\n",
    "    else:\n",
    "        if (i % 4000) not in indexes_to_skip:\n",
    "            flip_counter[\"only_flipped\"] += 1\n",
    "print(flip_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "and \t\t 0.16888267650442598\n",
      "as \t\t -0.04486665121424932\n",
      "also \t\t 0.10528351397815239\n",
      "after \t\t 0.045980802362786066\n",
      "when \t\t 0.2131291384002924\n",
      "but \t\t 0.0453525856646703\n",
      "before \t\t -0.05498259554867975\n",
      "although \t\t 0.3517490331870735\n",
      "then \t\t 0.02085213443815452\n",
      "while \t\t -0.04448240216588739\n",
      "so \t\t 0.27828441181821106\n",
      "because \t\t -0.15654184302159027\n",
      "still \t\t 0.10252030519320698\n",
      "if \t\t 0.05001933827924476\n",
      "though \t\t -0.03287692526911669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06988690150711298"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_pred = np.load(\"bert_predictions.npy\").ravel()\n",
    "conpono_pred = np.load(\"conpono_predictions.npy\").ravel()\n",
    "labels = np.load(\"labels.npy\").ravel()\n",
    "\n",
    "pos_index = np.argwhere(0 == labels) \n",
    "conpono_true = np.argwhere(conpono_pred == labels) \n",
    "bert_false = np.argwhere(bert_pred != labels) \n",
    "improvement = np.intersect1d(conpono_true, bert_false)\n",
    "improvement = np.intersect1d(improvement, pos_index)\n",
    "\n",
    "markers = set([\"but\", \"and\", \"as\", \"when\", \"if\", \"before\", \"because\",\n",
    "          \"while\", \"though\", \"after\", \"so\", \"although\", \"then\",\n",
    "          \"also\", \"still\"])\n",
    "from collections import Counter\n",
    "all_vecs_p = np.load(\"../tmp/BSO-BSO wiki-conpono-test-x.npy\")\n",
    "all_vecs_n = np.load(\"../tmp/BSO-BSO wiki-conpono-test-y.npy\")\n",
    "all_vecs = np.concatenate((all_vecs_p, all_vecs_n) )\n",
    "\n",
    "word_counter = Counter()\n",
    "cc = 0\n",
    "indexes_to_skip = []\n",
    "num_words = 0\n",
    "for i in list(improvement):\n",
    "    # print(i)\n",
    "    cc += 1\n",
    "    if cc % 100 == 0:\n",
    "        print(cc)\n",
    "    bert_vec = se.evaluation.embed['test']['X'][i]\n",
    "    for j in range(all_vecs.shape[0]):\n",
    "        if (all_vecs[j] == bert_vec).all():\n",
    "            sents = se.evaluation.data['test']['X'][j % 4000]\n",
    "            words = set([w.lower() for w in sents[0] ])\n",
    "            num_words += len(words)\n",
    "            indexes_to_skip.append(j)\n",
    "            if words.intersection(markers):\n",
    "                for w in words.intersection(markers):\n",
    "                    word_counter[w] += 1\n",
    "            break\n",
    "            \n",
    "general_word_counter = Counter()\n",
    "general_num_words = 0\n",
    "for j in range(len(se.evaluation.data['test']['X'])):\n",
    "#     if j in indexes_to_skip:\n",
    "#         continue\n",
    "    sents = se.evaluation.data['test']['X'][j]\n",
    "    words = set([w.lower() for w in sents[0] + sents[1]])\n",
    "    general_num_words += len(words)\n",
    "    if words.intersection(markers):\n",
    "        for w in words.intersection(markers):\n",
    "            general_word_counter[w] += 1\n",
    "\n",
    "counter = Counter()\n",
    "for w,c in word_counter.most_common():\n",
    "    gc = general_word_counter[w]\n",
    "    tmp_c = c / num_words\n",
    "    tmp_gc = gc / general_num_words\n",
    "    print(w, \"\\t\\t\", (tmp_c - tmp_gc) / tmp_gc)\n",
    "    counter[w] = (tmp_c - tmp_gc) / tmp_gc\n",
    "np.mean(list(counter.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-21 22:56:18,477 : Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2020-04-21 22:56:18,882 : https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/bert-base-uncased-config.json HTTP/1.1\" 200 0\n",
      "2020-04-21 22:56:18,888 : loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/daniter/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "2020-04-21 22:56:18,890 : Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2020-04-21 22:56:18,895 : Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2020-04-21 22:56:19,295 : https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/bert-base-uncased-vocab.txt HTTP/1.1\" 200 0\n",
      "2020-04-21 22:56:19,298 : loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/daniter/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2020-04-21 22:56:19,362 : ***** Transfer task : PDTB classification, task path: ../data/PDTB/Implicit *****\n",
      "\n",
      "\n",
      "2020-04-21 22:56:19,410 : Loaded 8693 instances\n",
      "\n",
      "2020-04-21 22:56:19,426 : Loaded 2972 instances\n",
      "\n",
      "2020-04-21 22:56:19,445 : Loaded 3024 instances\n",
      "\n",
      "2020-04-21 22:56:19,475 : encoding X to be: (8693, 3072)\n",
      "2020-04-21 22:56:19,486 : encoding X to be: (2972, 3072)\n",
      "2020-04-21 22:56:19,497 : encoding X to be: (3024, 3072)\n",
      "2020-04-21 22:56:19,497 : Training pytorch-MLP-nhid0-adam-bs64 with standard validation..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANITER False\n",
      "DANITER cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-21 22:56:20,636 : [('reg:1e-09', 43.1)]\n",
      "2020-04-21 22:56:20,637 : Validation : best param found is reg = 1e-09 with score             43.1\n",
      "2020-04-21 22:56:20,638 : Evaluating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANITER False\n",
      "DANITER cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-21 22:56:21,741 : Dev acc : 43.1 Test acc : 43.85 for PDTB\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   3   9   0   0  10   1   0   7   0   0]\n",
      " [  0  73 148   0   0 166   7   8  31   1   0]\n",
      " [  0  15 539   0   0  90  25   0  97   7   0]\n",
      " [  0   0  11   0   0   0   0   0   2   0   0]\n",
      " [  0   1  14   0   1   3   0   0  10   0   0]\n",
      " [  0  25 130   0   0 325  11  14  33   1   0]\n",
      " [  0   2  62   0   0  23 152   0  54   0   0]\n",
      " [  0   2  12   0   0  43   3  14   3   1   0]\n",
      " [  0  10 262   0   0 117  45   2 202   3   0]\n",
      " [  0   9  29   0   0  57   1   0  20  20   0]\n",
      " [  0   2  11   0   0  38   2   2   3   0   0]]\n",
      "{'PDTB-I': {'devacc': 43.1, 'acc': 43.85, 'ndev': 2972, 'ntest': 3024}}\n"
     ]
    }
   ],
   "source": [
    "# Set up logger\n",
    "PATH_TO_DATA = \"../data/\"\n",
    "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.DEBUG)\n",
    "config = BertConfig.from_pretrained('bert-{}-uncased'.format(\"base\"))\n",
    "config.output_hidden_states = True\n",
    "config.output_attentions = True\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-{}-uncased'.format(\"base\"))\n",
    "\n",
    "# model = BertModel.from_pretrained('../models/conpono/bert/', config=config).cuda()\n",
    "# model.eval()\n",
    "\n",
    "# Set params for SentEval\n",
    "params_discoeval = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 10, 'batch_size': 8,\n",
    "                    'tokenizer': tokenizer, \"layer\": \"pooler\", \"model\": \"base\", \"seed\": 111}\n",
    "params_discoeval['classifier'] = {'nhid': 0, 'optim': 'adam', 'batch_size': 64,\n",
    "                                 'tenacity': 5, 'epoch_size': 4}\n",
    "se = discoeval.engine.SE(params_discoeval, batcher, prepare)\n",
    "\n",
    "\n",
    "transfer_tasks = [\n",
    "    ['SParxiv'],\n",
    "    ['BSOwiki'],\n",
    "    ['DCwiki'],\n",
    "    ['PDTB-I']]\n",
    "    #['SParxiv', 'SProc', 'SPwiki'], \n",
    "    #['DCchat', 'DCwiki'],\n",
    "    #['BSOarxiv', 'BSOroc', 'BSOwiki'], \n",
    "    #['SSPabs', 'RST'],\n",
    "    #[ 'PDTB-E', 'PDTB-I']]\n",
    "    \n",
    "#se.evaluation = SPEval(os.path.join(PATH_TO_DATA, 'SP/arxiv'), \"Sentence Position arXiv\", nclasses=5, seed=111)\n",
    "#se.params.current_task = transfer_tasks[0][0]\n",
    "#se.evaluation.do_prepare(se.params, se.prepare)\n",
    "results = se.eval(transfer_tasks[3])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-102-3e2c8d916d4a>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-102-3e2c8d916d4a>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    262 - Expansion.Restatement 9, Contingency.Cause (2)\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "148 - Comparison.Contrast (1), Contingency.Cause (2)\n",
    "262 - Expansion.Restatement (8), Contingency.Cause (2)\n",
    "166 - Comparison.Contrast (1) , Expansion.Conjunction (5)\n",
    "130 - Expansion.Conjunction (5), Contingency.Cause (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.load(\"./pdtb_preds.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3024"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3024"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len((2)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = se.evaluation.data['test'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   3,   9,   0,   0,  10,   1,   0,   7,   0,   0],\n",
       "       [  0,   0, 148,   0,   0, 166,   7,   8,  31,   1,   0],\n",
       "       [  0,  15,   0,   0,   0,  90,  25,   0,  97,   7,   0],\n",
       "       [  0,   0,  11,   0,   0,   0,   0,   0,   2,   0,   0],\n",
       "       [  0,   1,  14,   0,   0,   3,   0,   0,  10,   0,   0],\n",
       "       [  0,  25, 130,   0,   0,   0,  11,  14,  33,   1,   0],\n",
       "       [  0,   2,  62,   0,   0,  23,   0,   0,  54,   0,   0],\n",
       "       [  0,   2,  12,   0,   0,  43,   3,   0,   3,   1,   0],\n",
       "       [  0,  10, 262,   0,   0, 117,  45,   2,   0,   3,   0],\n",
       "       [  0,   9,  29,   0,   0,  57,   1,   0,  20,   0,   0],\n",
       "       [  0,   2,  11,   0,   0,  38,   2,   2,   3,   0,   0]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(labels, preds)\n",
    "np.fill_diagonal(cm, 0)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It gets more mail in a month than McCall 's got in a year , and it 's not from mothers\n",
      "I feel about Sassy like I did about Working Woman 10 years ago\n",
      "########################################\n",
      "In addition , McCall 's put in a less than stellar performance\n",
      "Until a recent comeback , it saw steep losses in ad pages and circulation\n",
      "########################################\n",
      "his magazines will offer what many women 's magazines do n't\n",
      "We write straight for women on their level\n",
      "########################################\n",
      "We write straight for women on their level\n",
      "We do n't have passive readers\n",
      "########################################\n",
      "We do n't have passive readers\n",
      "that even Success , in part , fits the company 's image , since about 30 % of its readership is female\n",
      "########################################\n",
      "Besides , we have enough on our plate\n",
      "There is plenty of work to be done on what we have\n",
      "########################################\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for i, label  in enumerate(labels):\n",
    "\n",
    "    if label == 8 and preds[i] == 2:\n",
    "        c += 1\n",
    "        s1 = \" \".join(se.evaluation.data['test'][0][i])\n",
    "        s2 = \" \".join(se.evaluation.data['test'][1][i])\n",
    "        print(s1)\n",
    "        print(s2)\n",
    "        print(\"#\"*40)\n",
    "    if c > 5:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
